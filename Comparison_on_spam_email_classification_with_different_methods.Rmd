---
title: "Comparison on spam email classification with different methods"
subtitle: "732A92 Text Mining"
author: "Chao Fu(chafu696)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
csl: chaofu_format.csl
bibliography: fu_chao.bib
abstract: "Imbalanced data can have a significant influence on learning system. There are two methods in transforming imbalanced data into a balanced one, oversampling and undersampling. To process text using machine learning or Neural Network models, text data need to be encoded into vectors of numerical values. There are two typical methods for text processing, Term frequency–inverse document frequency(tf-idf) and word embedding. In this project, four cutting-edge models are applied which are Logistic Regression, Support Vector Machine, Random Forest and TextRNN(LSTM) to explore an optimal combination of methods and models for spam email classification. The results exhibit that (1) Undersampling can be used instead of the imbalanced one to obtain an optimal classification performance. (2) Although both undersampling and preprocessing by removing information on original data can have equivalent performance based on the Support Vector Machine model, undersampling can save human time. (3) Many steps should be applied to transform the data into a specific form that can be used in TextRNN(LSTM). Although both TextRNN(LSTM) and Support Vector Machine can perform on the same level in small data, the simple model, support vector machine, can be an optimal selection."
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, results = "markup")
```

```{r, include = FALSE, message = FALSE, warning = FALSE}
rm(list=ls())
library(tidyverse)
library(ggplot2)
library(fmsb)
library(gridExtra)
```

# 1 Introduction

Remote working is prevalent during the epidemic recently. Hence, there is a significant increase in the use of email. In this situation, spam email has attracted considerable attention. Generally, the number of spam emails is significantly less than non-spam email's which can lead to a problem of imbalanced data. There are two methods in transforming imbalanced data into a balanced one, oversampling and undersampling. To process text using machine learning or Neural Network models, text data need to be encoded into vectors of numerical values. There are two typical methods for text processing, Term frequency–inverse document frequency(tf-idf) and word embedding. Overall, an optimal combination of methods and models for spam email classification is facing challenges and it is worthwhile devoting much effort to this. This project aims to compare the discrepancies among the different methods in spam email classification and find an optimal combination. With this motive, two extra data, oversampling and undersampling, are created from the original one. Then tf-idf and word embedding matrix are determined for each data. In this project, four cutting-edge models are applied which are Logistic Regression, Support Vector Machine, Random Forest and TextRNN(LSTM). Finally, the classification accuracy on the test data is measured with these models based on different tf-idf and word embedding matrix. The results exhibit that (1) Original imbalanced data has the lowest accuracy and undersampling data ranks first in all models except Support Vector Machine. (2) In word embedding method, the TextRNN(LSTM) model ranks first in all three data compared with other ones. (3) In tf-idf method, Support Vector Machine ranks first in all three data compared with Logistic Regression and Random Forest. (4) In tf-idf method, Logistic Regression is the most sensitive to imbalanced data. (5) In word embedding method, Random Forest is the most sensitive to imbalanced data. (6) Support Vector Machine with tf-idf method and undersampling data in this project has the largest accuracy which is the same result as the kaggle solution reaching 96%. (7) Logistic Regression with tf-idf method and undersampling data in this project has a slight lower result than the kaggle solution(95% to 97%). (8) TextRNN(LSTM) with word embedding method and undersampling data in this project also has the largest accuracy reaching 96%. (9) Undersampling can be used instead of the imbalanced one to obtain an optimal classification performance. (10) Although both undersampling and preprocessing by removing information on original data can have equivalent performance based on the Support Vector Machine model, undersampling can save human time. (11) Many steps should be applied to transform the data into a specific form that can be used in TextRNN(LSTM). Although both TextRNN(LSTM) and Support Vector Machine can perform on the same level in small data, the simple model, support vector machine, can be an optimal selection.  

# 2 Background

### 2.1 Theory

**1) Oversampling and Undersampling** 

The performance obtained by the existing learning system can be affected by imbalanced labels in training data which means that the number of one label tremendously exceeds the other one. In this case, the learning system is facing challenges to learn the information behind the minority label. There are two non-heuristic methods to obtain balanced data by random selection from minority label's examples with replacement(oversampling) and from majority label's examples without replacement(undersampling)[@batista2004study].

Without losing any information from original data is the main merit of the oversampling method. However, it has many drawbacks such as extensive time consumption, serious overfitting risk, misleading information behind minority label. Although the undersampling method can save running time, it can lose some important information in the majority label.[@kaur2018comparing]

**2) Logistic Regression**

The conventional logistic regression equation is as follows:

$$P(Y_i) = \frac{e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + \dots \beta_pX_p}}$$

Using logit to transform the basic logistic regression into a form of multiple linear regression is given below:

$$logit(\hat{Y})=ln(p / 1 - p)= \beta_0 + \beta_1X_1 + \beta_2X_2 + \dots \beta_pX_p = \beta^TX$$

The interpretation of this equation is given below:

1) $p = P(Y_i)$ is the estimated probability of each response variable value occurring.

2) $\beta_0$ is the intercept which is regarded as a constant value. $X_1, X_2, \dots X_p$ are p number of explanatory variables. $\beta_1, \beta_2, \dots \beta_p$ are their regression coefficients. $\beta$ is a regression coefficient matrix$((p + 1) \times 1)$ and $X$ is a sample matrix$((p + 1) \times N)$ in which N is the number of observations.

**3) Support Vector Machine**

For binary classification, Support Vector Machine(SVM) is implemented by maximizing the margin to minimize the maximum loss[@boser1992training].

The hard SVM equation is given below[@deisenroth2020mathematics]:

$$\begin{aligned}
&\mathop{min}_{w,b} \frac{1}{2}\Vert w\Vert^2\\
&subject \; to \;y_n(w^TX + b) \ge 1 \; for \; all 
\;n = 1, 2, \dots, N\\
\end{aligned}$$

The interpretation of this equation is given below:

1) $b$ is the intercept which is regarded as a constant value.  $w$ is a vector $(p\times 1)$ normal to the hyperplane and $X$ is a sample matrix$(p \times N)$ in which N is the number of observations.

**4) Random Forest**

"Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest."[@breiman2001random]

**5) Long Short-Term Memory (LSTM)**

Hochreiter and Schmidhuber developed an innovative method called "Long Short-Term Memory"(LSTM) which can fix the limitation of recurrent neural network(RNN)[@hochreiter1997long]. Hence, the LSTM method is prevalent in text classification based on the context.

**6) TextRNN(LSTM)**

TextRNN is used for text classification with LSTM, which means bi-directional LSTM[@cai2018deeplearning].  

### 2.2 Method

**1) Creating four data**

The original data is imbalanced, in which the samples of Non-Spam heavily outnumber Spam. Random selection of the same number of Spam from Non-Spam samples without replacement to obtain undersampling data(balanced data). Then, the random selection of the same number of Non-Spam from Spam samples with replacement to obtain oversampling data(balanced data). With additional test data, four data are obtained, original(imbalanced data), oversampling(balanced data),  undersampling(balanced data)finally and test data.

**2) Creating pre-trained word embedding weights matrix**

The number of all three data is not sufficient to train the word embedding weights in TextRNN(LSTM) learning system. Hence, the pre-trained word embedding weights matrix is applied in its word embedding layer.  The original, oversampling and undersampling data generate pre-trained word embedding weights matrix respectively based on the spacy module. 

**3) Finding the best hyper-parameters**

For both word embedding and tf-idf method, the original data is applied to find the best hyper-parameters in Logistic Regression, Support Vector Machine and Random Forest model. Then, the best parameters are fixed in each of the three models. 

The pre-trained word embedding weights matrix is only used in TextRNN(LSTM). 70% of original data is used as training data, the rest is for validation data. Then, they are used to find the best epoch number based on the pre-trained word embedding weights matrix of original data. This process is also applied in oversampling and undersampling data.  


**4) Training models with best hyper-parameters and obtaining the accuracy on test data**

After the first 3 steps, 7 learning systems are obtained with the best hyper-parameters respectively. Then, these systems are trained by original, oversampling and undersampling data respectively. The accuracy of these trained systems based on test data which is unseen in the training process is measured.

# 3 Numerical example

This _[data](https://www.kaggle.com/datatattle/email-classification-nlp)_ was obtained from Kaggle.

The train and test data both have 2 variables : 

One is the response variable, a binary variable with two classes: "Spam" and "Non-Spam". The other is the explanatory variable with the email message.

Both train and test data don't contain missing data. However, the training data is imbalanced, "Spam" with 122, "Non-Spam" with 835. 

With the aim of this project, two other balanced data are created from the original data, one is undersampling and the other is oversampling.

# 4 Results

**Table 1: The accuracy of tf-idf**

|Method|Original_tf-idf|Oversampling_tf-idf|Undersampling_tfidf|
--- | --- | --- | --- |
|Logistic Regression|79%|90%|95%|
|Support Vector Machine|92%|90%|96%|
|Random Forest|82%|82%|87%|

Three models apply the tf-idf method. The values in the table are classification accuracy on the test data.

**Table 2: The accuracy of word-embedding**

|Method|original_word-embedding|oversampling_word-embedding|undersampling_word-embedding|
--- | --- | --- | --- |
|Logistic Regression|90%|92%|94%|
|Support Vector Machine|87%|89%|91%|
|Random Forest|82%|82%|94%|
|TextRNN(LSTM)|92%|93%|96%|

Four models apply the word embedding method. The values in the table are classification accuracy on the test data.

**Figure 1: Comparison on each model in three data**

```{r, echo = FALSE}
lr_acc <- tibble("name" = rep(c("Origin", "over", "under"), 2), "acc" = c(0.79, 0.9, 0.95, 0.9, 0.92, 0.94), "method" = rep(c("tf_idf", "word_em"), each = 3))
p1 = ggplot(lr_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Logistic Regression", x = "Sample catergory", y = "Accuracy") + 
  ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

svm_acc <- tibble("name" = rep(c("Origin", "over", "under"), 2), "acc" = c(0.92, 0.9, 0.96, 0.87, 0.89, 0.91), "method" = rep(c("tf_idf", "word_em"), each = 3))
p2 = ggplot(svm_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Support Vector Machine", x = "Sample catergory", y = "Accuracy") + ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

rf_acc <- tibble("name" = rep(c("Origin", "over", "under"), 2), "acc" = c(0.82, 0.82, 0.87, 0.82, 0.82, 0.94), "method" = rep(c("tf_idf", "word_em"), each = 3))
p3 = ggplot(rf_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Random Forest", x = "Sample catergory", y = "Accuracy") + 
  ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

lstm_acc <- tibble("name" = c("Origin", "over", "under"), "acc" = c(0.92, 0.93, 0.96), "method" = rep(c("word_em"), 3))
p4 = ggplot(lstm_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Text_RNN(LSTM)", x = "Sample catergory", y = "Accuracy") + 
  ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p1, p2, p3, p4, ncol = 2, nrow = 2)
```

Each model has a graph to show the comparison on the tf-idf and word embedding methods in original, oversampling and undersampling data.

**Figure 2: Comparison among models in three data**

```{r, echo = FALSE}
word_acc <- tibble("name" = rep(c("Origin", "over", "under"), 4), "acc" = c(0.9, 0.92, 0.94, 0.87, 0.89, 0.91, 0.82, 0.82, 0.94, 0.92, 0.93, 0.96), "method" = rep(c("Logistic Regression", "Support Vector Machine", "Random Forest", "TextRNN(LSTM)"), each = 3))
p5 = ggplot(word_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Word_embedding", x = "Sample catergory", y = "Accuracy") + 
  ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

tf_acc <- tibble("name" = rep(c("Origin", "over", "under"), 3), "acc" = c(0.79, 0.90, 0.95, 0.92, 0.90, 0.96, 0.82, 0.82, 0.87), "method" = rep(c("Logistic Regression", "Support Vector Machine", "Random Forest"), each = 3))
p6 = ggplot(tf_acc, aes(x = name, y = acc, color = method, fill = method)) + geom_bar(stat = "identity", position = "dodge", width = 0.5) + 
  labs(title = "Tf_idf", x = "Sample catergory", y = "Accuracy") + 
  ylim(0, 1) + theme(plot.title = element_text(hjust = 0.5))

grid.arrange(p5, p6, ncol = 1, nrow = 2)
```

Each method has a graph to show the comparison on the four different models in original, oversampling and undersampling data.

# 5 Discussion

(1) Imbalanced data has the lowest accuracy and undersampling data ranks first in all models except Support Vector Machine. 

(2) In word embedding method, TextRNN(LSTM) model ranks first in all three data compared with other ones. 

(3) In tf-idf method, Support Vector Machine model ranks first in all three data compared with Logistic Regression and Random Forest.

(4) In tf-idf method, Logistic Regression is the most sensitive to imbalanced data. 

(5) In word embedding method, Random Forest is the most sensitive to imbalanced data. 

(6) The _[solution](https://www.kaggle.com/lvalencia/get-rid-of-spam)_  based on the same data is published in kaggle. This solution used Natural Language ToolKit(NLTK) module. The spacy module is used in this project. 

(7) The kaggle solution processes the original data by removing some information.  However, this process can lose some potentially important information. Hence, this project does not remove any information from the original data. Moreover, the kaggle solution only used original data(imbalanced data) and tf-idf method but three different data and two different methods are used in this project.

(8) Support Vector Machine with tf-idf method and undersampling data in this project has the largest accuracy which is the same result as the kaggle solution reaching 96%.

(9) Logistic Regression with tf-idf method and undersampling data in this project has slightly lower results than the kaggle solution(95% to 97%).

(10) TextRNN(LSTM) with word embedding method and undersampling data in this project also has the largest accuracy reaching 96%.

# 6 Conclusion

(1) Undersampling can be used instead of the imbalanced one to obtain an optimal classification performance.

(2) Although both undersampling and preprocess by removing information on original data can have equivalent performance based on the Support Vector Machine model
, undersampling can save human time. 

(3) Many steps should be applied to transform the data into a specific form that can be used in TextRNN(LSTM). Although both TextRNN(LSTM) and Support Vector Machine can perform on the same level in small data, the simple model, support vector machine should be an optimal selection.

# 7 References
